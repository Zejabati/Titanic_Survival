# -*- coding: utf-8 -*-
"""ِHW2_95103764_95103978_95103707.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sqf_xArqwib_NSxdaahQGW15Q7E-RGwY

**1**
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import SimpleImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

from sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier,BaggingClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn import preprocessing

from sklearn.metrics import confusion_matrix,precision_score,recall_score,f1_score,accuracy_score

# load data
data = pd.read_csv('Titanic.csv')
data.head()

print(data.describe())
print(data.shape)
print(data.columns)

"""**2**"""

null_counts = data.isnull().sum()
#null
print(null_counts[null_counts > 0].sort_values(ascending=False))
#percent
print((null_counts[null_counts > 0].sort_values(ascending=False)/len(data))*100)

#Sex male1 female0
data['Sex'] = data['Sex'].replace(['female','male'],[0,1])

# Remove some features
data=data.drop(columns=['Name','PassengerId','Ticket','Cabin'])
data.head()

f, ax = plt.subplots(figsize = [10,9])
sns.heatmap(data.corr(),linewidths = .5, annot = True, cmap = 'YlGnBu', square = True)

# find most frequent Embarked value and store in variable
most_embarked = data.Embarked.value_counts().index[0]
# fill NaN with most_embarked value
data.Embarked = data.Embarked.fillna(most_embarked)
data.head()

# create dummy variables for categorical features(One Hot Encoding)
data = pd.get_dummies(data,columns=['Embarked'])
data.head()

SingleImputer = IterativeImputer(missing_values=np.nan)
DfSingle = data.copy(deep=True)
DfSingle.iloc[:,:] = SingleImputer.fit_transform(DfSingle)

MeanImputer = SimpleImputer(missing_values=np.nan, strategy='mean')
DfMean = data.copy(deep=True)
DfMean.iloc[:, :] = MeanImputer.fit_transform(DfMean)

IterativeImputer = IterativeImputer(missing_values=np.nan, sample_posterior=True, min_value=0,random_state=0)
DfIterative = data.copy(deep=True)
DfIterative.iloc[:, :] = IterativeImputer.fit_transform(DfIterative)

data['Age'].plot(kind='kde', c='red')
DfMean['Age'].plot(kind='kde')
DfIterative['Age'].plot(kind='kde', c='yellow')
DfSingle['Age'].plot(kind='kde', c='green')
labels = ['Baseline (Initial Case)', 'Mean Imputation', 'Iterative Imputation','MICE Imputation']
plt.legend(labels)
plt.show()

data = DfIterative.copy(deep=True)
 data.head(10)

data['Fare']= data['Fare'].replace(0,np.nan)
null_counts = data.isnull().sum()
#null
print(null_counts[null_counts > 0].sort_values(ascending=False))
#percent
print((null_counts[null_counts > 0].sort_values(ascending=False)/len(data))*100)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
from sklearn.impute import SimpleImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

SingleImputer = IterativeImputer(missing_values=np.nan)
DfSingle = data.copy(deep=True)
DfSingle.iloc[:,:] = SingleImputer.fit_transform(DfSingle)

MeanImputer = SimpleImputer(missing_values=np.nan, strategy='mean')
DfMean = data.copy(deep=True)
DfMean.iloc[:, :] = MeanImputer.fit_transform(DfMean)

IterativeImputer = IterativeImputer(missing_values=np.nan, sample_posterior=True, min_value=0,random_state=0)
DfIterative = data.copy(deep=True)
DfIterative.iloc[:, :] = IterativeImputer.fit_transform(DfIterative)

data['Fare'].plot(kind='kde', c='red')
DfMean['Fare'].plot(kind='kde')
DfIterative['Fare'].plot(kind='kde', c='yellow')
DfSingle['Fare'].plot(kind='kde', c='green')
labels = ['Baseline (Initial Case)', 'Mean Imputation', 'Iterative Imputation','MICE Imputation']
plt.legend(labels)
plt.show()

data= DfIterative.copy(deep=True)

plot=data.boxplot(column='Fare',by='Pclass',figsize=(7,7))
print(plot)
Table_Fare=pd.pivot_table(data,index=['Pclass'],values=['Fare'],aggfunc=np.mean)
dictionary_Fare={'1':Table_Fare['Fare'][1],'2':Table_Fare['Fare'][2],'3':Table_Fare['Fare'][3]}

for i in range(len(data)):
  if data['Fare'][i]==0:
    if(data['Pclass'][i]==1):
      data['Fare'][i] = dictionary_Fare['1']
    elif (data['Pclass'][i]==2):
      data['Fare'][i] = dictionary_Fare['2']
    else:
      data['Fare'][i] = dictionary_Fare['3']

# create dummy variables for categorical features(One Hot Encoding)
data = pd.get_dummies(data,columns=['Pclass'])
data.head()

f, ax = plt.subplots(figsize = [10,9])
sns.heatmap(data.corr(),linewidths = .5, annot = True, cmap = 'YlGnBu', square = True)

"""**3**"""

y=data['Survived']
x=data.drop(columns=['Survived'])
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)

"""**KNN**"""

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

print('accuracy:',knn.score(X_test, y_test))
print('recall:',recall_score(y_test, knn.predict(X_test)))
print('precision:',precision_score(y_test, knn.predict(X_test)))
print('f1_score:',f1_score(y_test, knn.predict(X_test)))
print('confusion_matrix:','\n',confusion_matrix(y_test, knn.predict(X_test)))

score_knn = []
weights = ['uniform', 'distance']
for i in np.arange(1, 11, 1):
  for j in np.arange(1, 5, 1):
    for k in weights:
      knn = KNeighborsClassifier(n_neighbors=i, p=j, weights=k)
      knn.fit(X_train, y_train)
      score_knn.append([i, j, k, np.mean(cross_val_score(knn, X_train, y_train, scoring='accuracy',cv=5))])

score_knn = pd.DataFrame(score_knn)
score_knn = score_knn.sort_values(by=3, ascending=False).reset_index()
i=score_knn[0][0]
j=score_knn[1][0]
k=score_knn[2][0]
print('best parameters:','n_neighbors:',i ,'p:',j ,'weights:',k)
knn = KNeighborsClassifier(n_neighbors=i, p=j, weights=k)
knn.fit(X_train, y_train)
print('accuracy:',knn.score(X_test, y_test))
print('recall:',recall_score(y_test, knn.predict(X_test)))
print('precision:',precision_score(y_test, knn.predict(X_test)))
print('f1_score:',f1_score(y_test, knn.predict(X_test)))
print('confusion_matrix:','\n',confusion_matrix(y_test, knn.predict(X_test)))

"""**Decision Tree**"""

dectree = tree.DecisionTreeClassifier(max_depth=5)
dectree.fit(X_train, y_train)
tree.plot_tree(dectree.fit(X_train, y_train))
plt.show()

print('accuracy:',dectree.score(X_test, y_test))
print('recall:',recall_score(y_test, dectree.predict(X_test)))
print('precision:',precision_score(y_test, dectree.predict(X_test)))
print('f1_score:',f1_score(y_test, dectree.predict(X_test)))
print('confusion_matrix:','\n',confusion_matrix(y_test, dectree.predict(X_test)))

score_dectree = []
for i in np.arange(3, 15, 1):
  dectree = tree.DecisionTreeClassifier(max_depth=i)
  dectree.fit(X_train, y_train)
  score_dectree.append([i, np.mean(cross_val_score(dectree, X_train, y_train,scoring='accuracy',cv=5))])

score_dectree = pd.DataFrame(score_dectree)
score_dectree = score_dectree.sort_values(by=1, ascending=False).reset_index()
i=score_dectree[0][0]
print('best parameter:','max_depth:',i)

dectree = tree.DecisionTreeClassifier(max_depth=i)
dectree.fit(X_train, y_train)
tree.plot_tree(dectree.fit(X_train, y_train))
plt.show()
print('accuracy:',dectree.score(X_test, y_test))
print('recall:',recall_score(y_test, dectree.predict(X_test)))
print('precision:',precision_score(y_test, dectree.predict(X_test)))
print('f1_score:',f1_score(y_test, dectree.predict(X_test)))
print('confusion_matrix:','\n',confusion_matrix(y_test, dectree.predict(X_test)))

"""**Naive Bayes**"""

gnb = GaussianNB()
gnb.fit(X_train, y_train)

print('accuracy:',gnb.score(X_test, y_test))
print('recall:',recall_score(y_test, gnb.predict(X_test)))
print('precision:',precision_score(y_test, gnb.predict(X_test)))
print('f1_score:',f1_score(y_test, gnb.predict(X_test)))
print('confusion_matrix:','\n',confusion_matrix(y_test, gnb.predict(X_test)))

"""**Logistic** **Regression**"""

X_train = preprocessing.scale(X_train)
X_test = preprocessing.scale(X_test)
C_param_range = [0.001,0.01,0.1,1,10,100]

score_log = []
for i in C_param_range:
  lr_model = LogisticRegression(penalty='l2',C=i, random_state=0)
  lr_model.fit(X_train,y_train)
  score_log.append([i, np.mean(cross_val_score(lr_model, X_train, y_train,scoring='accuracy',cv=5))])

score_log = pd.DataFrame(score_log)
score_log = score_log.sort_values(by=1, ascending=False).reset_index()
i=score_log[0][0]
print('best parameter:','C:',i)

lr_model = LogisticRegression(penalty='l2',C=i, random_state=0,)
lr_model.fit(X_train,y_train)

print('accuracy:',lr_model.score(X_test, y_test))
print('recall:',recall_score(y_test, lr_model.predict(X_test)))
print('precision:',precision_score(y_test, lr_model.predict(X_test)))
print('f1_score:',f1_score(y_test, lr_model.predict(X_test)))
print('confusion_matrix:','\n',confusion_matrix(y_test, lr_model.predict(X_test)))

"""**Bagging**"""

bag = BaggingClassifier(n_estimators=100)
bag.fit(X_train, y_train)

print('accuracy:',bag.score(X_test, y_test))
print('recall:',recall_score(y_test, bag.predict(X_test)))
print('precision:',precision_score(y_test, bag.predict(X_test)))
print('f1_score:',f1_score(y_test, bag.predict(X_test)))
print('confusion_matrix:','\n',confusion_matrix(y_test, bag.predict(X_test)))

score_bag = []
for i in np.arange(5, 110, 5):
  bag = BaggingClassifier(n_estimators=i)
  bag.fit(X_train, y_train)
  score_bag.append([i, np.mean(cross_val_score(bag , X_train, y_train,scoring='accuracy',cv=5))])

score_bag = pd.DataFrame(score_bag)
score_bag = score_bag.sort_values(by=1, ascending=False).reset_index()
i=score_bag[0][0]
print('best parameter:','n_estimators:',i)
bag = BaggingClassifier(n_estimators=i)
bag.fit(X_train, y_train)

print('accuracy:',bag.score(X_test, y_test))
print('recall:',recall_score(y_test, bag.predict(X_test)))
print('precision:',precision_score(y_test, bag.predict(X_test)))
print('f1_score:',f1_score(y_test, bag.predict(X_test)))
print('confusion_matrix:','\n',confusion_matrix(y_test, bag.predict(X_test)))

"""**Random Forest**"""

rf = RandomForestClassifier()
rf.fit(X_train, y_train)

print('accuracy:',rf.score(X_test, y_test))
print('recall:',recall_score(y_test, rf.predict(X_test)))
print('precision:',precision_score(y_test, rf.predict(X_test)))
print('f1_score:',f1_score(y_test, rf.predict(X_test)))
print('confusion_matrix:','\n',confusion_matrix(y_test, rf.predict(X_test)))

score_rf = []
max_features = ['auto', 'sqrt']

for i in np.arange(100, 501, 100):
  for j in np.arange(5, 21 , 5):
    for k in max_features:
      rf = RandomForestClassifier()
      rf.fit(X_train, y_train)
      score_rf.append([i, j, k, np.mean(cross_val_score(rf, X_train, y_train, scoring='accuracy',cv=5))])

score_rf = pd.DataFrame(score_rf)
score_rf = score_rf.sort_values(by=3, ascending=False).reset_index()
i=score_rf[0][0]
j=score_rf[1][0]
k=score_rf[2][0]
print('best parameters:','n_estimators:',i ,'max_depth:',j ,'max_features:',k)

rf = RandomForestClassifier(n_estimators=i, max_depth=j, max_features=k)
rf.fit(X_train,y_train)

print('accuracy:',rf.score(X_test, y_test))
print('recall:',recall_score(y_test, rf.predict(X_test)))
print('precision:',precision_score(y_test, rf.predict(X_test)))
print('f1_score:',f1_score(y_test, rf.predict(X_test)))
print('confusion_matrix:','\n',confusion_matrix(y_test, rf.predict(X_test)))

"""**Support Vector Machines**"""

parameters_svm = {'C':[0.9,0.01],'kernel':['rbf','linear'], 'gamma':[0,0.1,'auto'], 'probability':[True,False],'degree':[3,4,10]}
clf_svm = SVC()

def grid(model,parameters):
    grid = GridSearchCV(estimator = model, param_grid = parameters, cv = 10, scoring = 'accuracy')
    grid.fit(X_train,y_train)
    return grid.best_score_, grid.best_estimator_.get_params()

best_score_svm, best_params_svm = grid(clf_svm, parameters_svm)
print(best_score_svm)
print(best_params_svm)

clf_svm = SVC(C=0.9,kernel='rbf',gamma='auto')
clf_svm.fit(X_train,y_train)

print('accuracy:',clf_svm.score(X_test, y_test))
print('recall:',recall_score(y_test, clf_svm.predict(X_test)))
print('precision:',precision_score(y_test, clf_svm.predict(X_test)))
print('f1_score:',f1_score(y_test, clf_svm.predict(X_test)))
print('confusion_matrix:','\n',confusion_matrix(y_test, clf_svm.predict(X_test)))